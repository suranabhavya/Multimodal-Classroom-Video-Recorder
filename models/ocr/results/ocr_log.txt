[Frame 4770]
Transformers — Part 2

LADS ~ Spring 2025

[Frame 7680]
Transformers — Part 2

DDLADS= Spring 2025,

[Frame 7690]
~ ‘

[Frame 7790]
Transformers — Part 2

LADS Spring 2025

[Frame 8270]
Spring 2025 DLADS Midterm Challenge

[Frame 20310]
| Spring 2025 DLA4DS Midterm Challenge

[Frame 20350]
Transformers — Part 2

‘LADS ~ Spring 2025

[Frame 20450]
| Spring 2025 DL4DS Midterm Challenge

[Frame 20510]
Spring 2025 DL4DS Midterm Challenge

e--

Basic Details ee
'

[Frame 20520]
Spring 2025 DLADS Midterm Challenge

[Frame 20970]
© Leaderboard

[Frame 21020]
Transformers — Part 2

LADS ~ Spring 2025

[Frame 21160]
BOSTON
UN 4

Transformers — Part 2

DLADS - Spring 2025

[Frame 21220]
Transformers — Part 2

LADS — Spring 2025

[Frame 21230]
Transformers — Part 2

DL4DS - Spring 2025

[Frame 25790]
Recap From Part 1

* Motivation

* Dot-product self-attention

* Applying Self-Attention

* The Transformer Architecture

* Three Types of NLP Transformer
Models
* Encoder
* Decoder
* Encoder-Decoder

[Frame 26440]
Transformers

* Motivation
* Dot-product self-attention

* Applying Self-Attention

* The Transformer Architecture
“Th

[Frame 26760]
Join at
slido.com
#4111039

hens pot

Which model flavor do you use for Named Entity Recognition?

Encoder

Decoder

Encoder-Decoder

[Frame 37070]
3 Types of Transformer Models

~ transforms text embeddings into representations that
support variety of tasks (e.g. sentiment analysis, classification)
‘% Model Example: BERT

— predicts the next token to continue the input text (e.g.
ChatGPT, Al assistants)
+ Model Example: GPT4, GPTa

— used in sequence-to-sequence tasks, where one
text string is converted to another (e.g. machine translation)

[Frame 37810]
Next Token Selection

[Frame 37850]
Next Token Selection

+ Recall: output is a [V|x1 vector of probabilities
+ How should we pick the next token?
+ Trade off between accuracy and diversity

[Frame 40100]
Next Token Selection Probability of

target token

Recall: output is a |V|x1 vector of probabilities LL

Selectin methods:
* Greedy selection
* Top-K

+ Nucleus

+ Beam search

[Frame 40460]
Probablity of

Next Token Selection — Greedy target token
oo 4

Pick most likely token (greedy)

i, = argmax [Pr(y, outputs = model (inputs)

value, index = outputs.nax(1)
Might pick first token yo, but then there is no y, where Pr(y| yo)
is high.

Result is generic and predictable. Same output for a given input
context

[Frame 45170]
Next Token Selection -- Sampling Probability of

target token

Sample from the probability distribution I m

AN

Will occasionally sample from the long tail of the distribution,
producing some unlikely word combinations

[Frame 48460]
. Probability of

Next Token Selection — Top K Sampling target token
aon a

1. Generate the probability vector as usual

2. Sort tokens by likelihood

3. Discard all but top k most probable words

4

Renormalize the probabilities to be valid probability distribution
(e.g. sum to 1)

. Sample from the new distribution

Depends on the distribution. Could be low variance, reducing diversity

[Frame 51630]
- . Probabiity of
Next Token Selection — Nucleus Sampling —_ weet toten
=o
Instead of keeping top-k, keep the top p percent of the .
probability mass.

Choose from the smallest set from the vocabulary such
that
YS Powiwer) > p

wevir)

Depends on the distribution. Could be low variance,
reducing diversity

[Frame 54650]
Next Token Selection — Beam Search
Commonly used in machine
translation

Maintain multiple output choices
and then choose best combinations
later via tree search

V = {yes, ok, <eos>}
We want to maximize p(ty, tz, t3).

Greedy: 0.5x0.4x1.0 = 0.20

[Frame 61180]
Next Token Selection — Beam Search

But we can’t exhaustively search the entire vocabulary
Keep k tokens (beam width) at each step

[Frame 61410]
Next Token Selection — Beam Search 208: Beginning of

Keep k tokens at each step

Eg.k=2

Prune to k at each step

[Frame 63230]
Next Token Selection — Beam Search (k=2)

Calculated with fog
probabilities and add Pick the top 2 tokens.

[Frame 69770]
Next Token Selection — Beam Search

We have 2 paths randomly
terminating at EOS with
same cumulative log
probabilities.

Randomly pick 1.

[Frame 73330]
Next Token Selection

* Greedy selection
* Top-K

+ Nucleus

+ Beam search

[Frame 73680]
Context Length of LLMs
Large Language Model Context Size

15 P10

,
lama 2 32K
¢
rr 3k
GPrA Ture, 128K
ama 3. ‘
Claude 35 200K
Sonnet ¢ |
Gootle Gemini Milions ee eee | |

[Frame 75810]
Attention Matrix

W

Reve

Scales quadratically with
sequence length N, e.g. Ni

[Frame 82930]
Masked Attention

a)

b)

~1/2 the interactions but
still scales quadratically

[Frame 83860]
Use Convolutional Structure in Attention

a)

Wey Keys

Encoder Decoder
(non-causal) (causal)

[Frame 87150]
Have some tokens interact g

Decoder Encoder

[Frame 93690]
Tokenization and Word
Embedding

[Frame 94980]
Tokenizer

character (ew
Unicode)
strings)

character (ea.
Unicode)
strings

chooses input “units”, e.g. words, sub-words, characters via tokenizer
training

In tokenizer training, commonly occurring substrings are greedily merged based on
their frequency, starting with character pairs

[Frame 98440]
Tokenization Issues

+ Wy stim won at rans rguges (eg apanes) Toeration
+ Wy GP have more tan necessary woul ang in Phen? Theat,

[Frame 101550]
Unicode Standard and UTF-8

* Unicode - variable length character encoding standard. currently defines 149,813
characters and 161 scripts, including emoji, symbols, et.

* Unicode Codepoint — can represent up to 17x26 = 1,114,112 entries. e.g
U+0000 ~ U+1OFFFF in hexadecimal

* Unicode Transformation Standard (e.g. UTF-8) ~is a variable length encoding

using one to four bytes
+ First 128 chars same as ASCIL

[Frame 106510]
Tokenizer

Two common tokenizers:
* Byte Pair Encoding (BPE) ~ Used by OpenAl GPT2, GPT4, etc.
* The BPE algorithm is "byte-level” because it runs on UTF-8 encoded strings.

* This algorithm was popularized for LLMs by the GPT-2 paper and the
associated GPT-2 code release from OpenAl. Sennrich et al. 2015 is cited as,

the original reference for the use of BPE in NLP applications. Today, all
modern LLMs (e.g. GPT, Llama, Mistral) use this algorithm to train their
tokenizers.*

+ sentencepiece

+ (e.g. Llama, Mistral) use sentencepiece instead. Primary difference being that

sentencepiece runs BPE directly on Unicode code points instead of on UTF-8
encoded bytes.

[Frame 108370]
BPE Pseudocode

Initialize vocabulary + individual characters in
the text and their frequencies
While desired vocabulary size not reached

Identify the most frequent pair of adjacent

tokens/characters in the vocabulary
Merge this pair to form a new token
Update the vocabulary with this new token

[Frame 111400]
Enforce a Token Split Pattern

er2_spurt_partean @stsoaty I velre) | Pp(LIs1 B41
PUNTDILI BIND] #15215) Ns

GGPTA_SPLIT PATTERN = °°7° (24: {sdnth [U1 vel re) (°\r\n\pCL)\PO)12+\PKL)*1\B(00 (1.39
PEAVS\pAL}WpAN}] [Arn] +] \ 82 (rin [VS C2 48) 8

* Do not allow tokens to merge across certain characters or patterns
+ Common contraction endings: ‘l, ‘ve, ‘re

+ Match words with a leading space

* Match numeric sequences

* carriage returns, new lines

[Frame 113640]
GPT4 Tokenizer 110k, bases the GPT4 tokenizer

Tiktokenizer

[Frame 115820]
GPT2 Tokenizer

Tiktokenizer

You can see some issues with the
GPT2 tokenizer with respect to
python code

[Frame 117370]
GPT4 Tokenizer

Tiktokenizer

Issues are improved with GPT4
tokenizer

[Frame 118150]
a)

Sith nea Byte Pair Encoding (BPE) Example

[Frame 120450]
Generally of tokens increases an
then easing ater
continuing to merge tokens

[Frame 121640]
Learned Embeddings

‘Some txt strog> earned
Embedings: +
near Layer

After the tokenizer, you have an updated "vocabulary” indexed by token ID
Next step is to translate the token into an embedding vector

Translation is done via a linear layer which is typically learned with the rest of the
transformer model

self.embedding = on.Enbedding(vocab_size, enbedding_dim)
Special layer definition, likely to exploit sparsity of input

[Frame 126710]
Embeddings Output

inthis example, we are
assuming a token is simaly 8
complete word

Input. X. Vocabulary embedaings. 2,

pial embedding ste, 0, is 1024
Typical vocabulary sz, |, 1 30,000 Token indices, T
0 30M parameters just fr ths matrix

[Frame 128250]
Next Jupyter Notebook assignment

+ will release shortly

>self-attention
> multi-head self-attention

[Frame 128320]
Embeddings Output

inthis example, we are
assuming a token Is simply 8
complete word

Input. X. Vocabulary embedcings, 9,

+ Typical embedding sie, 1024
+ ypical vocabulary size, V1, s 30,000 Token indices, T
+ 50304 parameters just fortis matrial

[Frame 128340]
Next Jupyter Notebook assignment

+ will release shortly

>self-attention
> multi-head self-attention

[Frame 131420]
Next Feedback

+ Image Transformers [O)

+ Multimodal Transformers

[Frame 131730]
Next Feedback

+ Image Transformers
+ Multimodal Transformers

[Frame 132070]
Next Feedback

+ Image Transformers
+ Multimodal Transformers

[Frame 132110]
Next Feedback

+ Image Transformers
+ Multimodal Transformers

[Q}

0}
4
0}
